{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAq38CagTgwj+6SA8pn2sG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greymattar/ECG_WGAN_AugmentationClassification/blob/main/ML_models_GenTest_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHpgpsvVfWWk",
        "outputId": "e0bfdc9f-ab43-478b-d47c-7f6afcce0a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/gdrive')\n",
        "import sys\n",
        "import os\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "os.chdir('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "myPath_base = os.path.join(\"PycharmProjects\\\\paper2_gen_data\\\\\", \"MIT_BIH\", \"multiclass\", \"01_cond\\\\screened\",\"gb_L_cond_screened.json\")\n",
        "df =pd.read_json(myPath_base)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lrqLwVdmeN_",
        "outputId": "f788e9c1-fbbc-4f41-83dc-79732ea59c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0         1         2         3         4         5         6    \\\n",
            "0 -0.596866 -0.572873 -0.558806 -0.579448 -0.579438 -0.526910 -0.563536   \n",
            "1 -0.713581 -0.718979 -0.690804 -0.733377 -0.730963 -0.695825 -0.726390   \n",
            "2 -0.237402  0.010355 -0.027357  0.109423  0.031767  0.177342  0.141292   \n",
            "3 -0.700903 -0.709553 -0.674504 -0.719653 -0.720079 -0.684136 -0.719242   \n",
            "4 -0.677850 -0.699551 -0.647607 -0.694001 -0.697317 -0.658262 -0.709017   \n",
            "\n",
            "        7         8         9    ...       246       247       248       249  \\\n",
            "0 -0.518285 -0.518867 -0.498051  ... -0.402239 -0.387153 -0.387750 -0.432009   \n",
            "1 -0.687446 -0.695721 -0.681294  ... -0.335653 -0.322302 -0.344544 -0.414038   \n",
            "2  0.192209  0.162326  0.182625  ... -0.466923 -0.553367 -0.527034 -0.506896   \n",
            "3 -0.681613 -0.694466 -0.678787  ... -0.330393 -0.320869 -0.338802 -0.412933   \n",
            "4 -0.677262 -0.704213 -0.680731  ... -0.330031 -0.339077 -0.343634 -0.425239   \n",
            "\n",
            "        250       251       252       253       254       255  \n",
            "0 -0.471937 -0.458714 -0.491670 -0.504464 -0.498487 -0.575354  \n",
            "1 -0.443119 -0.452657 -0.477844 -0.493369 -0.478432 -0.556066  \n",
            "2 -0.514989 -0.526073 -0.536538 -0.589303 -0.536391 -0.620032  \n",
            "3 -0.442579 -0.451664 -0.476646 -0.493571 -0.482795 -0.560650  \n",
            "4 -0.457589 -0.463061 -0.483344 -0.507480 -0.506744 -0.563389  \n",
            "\n",
            "[5 rows x 256 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "classes2keep = ['N','/', 'A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "\n",
        "data = []  # List to store the beat values and class labels\n",
        "\n",
        "for cl in classes2keep_folder:\n",
        "\n",
        "    filename = \"gb_{}_cond_screened.json\".format(cl)\n",
        "    file_path = os.path.join(\"PycharmProjects\\\\paper2_gen_data\\\\\", \"MIT_BIH\", \"multiclass\", \"01_cond\\\\screened\", filename)  # Replace with the actual path to the JSON files\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            json_data = json.load(file)\n",
        "\n",
        "            # Extract beat values and class label\n",
        "            for row in json_data:\n",
        "                beat_values = row\n",
        "                class_label = cl\n",
        "\n",
        "                data.append([beat_values, class_label])\n",
        "\n",
        "# Print the joined data\n",
        "\n",
        "print(data[1][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adh5cNgx_yLw",
        "outputId": "f56e5cf8-6c1c-44a1-a9e8-b49c9576bf61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[15000][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JazuyRg3k8cm",
        "outputId": "e2d5cc21-2428-4a4e-f4b3-80e3af317a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save data as JSON\n",
        "output_file = 'gb_together.json'  # Replace with the desired path and filename for the output JSON file\n",
        "\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(data, file)\n",
        "\n",
        "print(\"Data saved successfully to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1l_jwBAKQnV",
        "outputId": "c0f673d9-2736-40c9-d092-d8b5b8d855b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved successfully to gb_together.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20MzfxzpmO6Q",
        "outputId": "c6bbb7fe-fddc-4d0c-897a-2304289d6e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/160.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import csv\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "\n",
        "from ekg_class import dicts\n",
        "from model_ac_wgan_gp_ecg import Disc_ac_wgan_gp_1d, Gen_ac_wgan_gp_1d, initialize_weights_1d\n",
        "from utils import gradient_penalty, normalize, grid_plot_save, grid_plot\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.datetime.now()\n",
        "print((\"\\n\" + \"*\" * 50 + \"\\n\\t\\tstart time:      {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 50).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "#drive = \"F:\"\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "myPath_dataset = os.path.join(myPath_base, \"\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dry_run = False\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 12\n",
        "BATCH_SIZE = 16\n",
        "CHANNELS_IMG = 1\n",
        "CRITIC_ITERATIONS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "GEN_EMBEDDING = 100\n",
        "LAMBDA_GP = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "Z_DIM = 100\n",
        "\n",
        "tr_ts_ratio = 0.9     # \"training set / test set\" split ratio\n",
        "len_ratio = 1     # to study the effect of support set number of samples (shorter train sets)\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', \"\")\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# transforms = transforms.Compose(\n",
        "#    [\n",
        "#        transforms.Resize(IMG_SIZE),\n",
        "#        transforms.ToTensor(),\n",
        "#        transforms.Normalize(\n",
        "#            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "#    ]\n",
        "# )\n",
        "\n",
        "\n",
        "with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "all_classes = ['/', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "all_classes_folder = ['P', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "\n",
        "all_classes.remove('Q')\n",
        "all_classes.remove('S')\n",
        "# all_classes.remove('V')      # why remove this? plots show no-uniform pattern\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%    begin classes statistics      %%%%%%%%%%%%%%%%%%\n",
        "\"\"\"\n",
        "stats_all_classes =\n",
        "        {\n",
        "        'N': 75052,   'L': 8075,  'R': 7259,  'V': 7130,\n",
        "        '/': 7028,    'A': 2546,  'f': 982,   'F': 803,\n",
        "        'j': 229,     'a': 150,   'E': 106,   'J': 83,\n",
        "        'Q': 33,      'e': 16,    'S': 2\n",
        "        }\n",
        "P: Paced beat\n",
        "A: Atrial Premature contraction\n",
        "E: Ventricular Escape beat\n",
        "F: Fusion of Ventricular and Normal beat\n",
        "J: Nodal (junctional) Premature Beat\n",
        "L: Left bundle branch block beat\n",
        "N: Normal beat\n",
        "Q: Unclassifiable beat\n",
        "R: Right bundle branch block beat\n",
        "S: Premature or ectopic supraventricular beat\n",
        "V: Premature Ventricular Contraction\n",
        "a: Aberrated Atrial Premature beat\n",
        "e: Atrial escape beat\n",
        "f: Fusion of paced and normal beat\n",
        "j: Nodal (junctional) escape beat\n",
        "\"\"\"\n",
        "# %%%%%%%%%%%%%%%%%    end classes statistics    %%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "d_set = \"MIT_BIH\"\n",
        "# Note: '/' and 'P' are the same\n",
        "#classes2keep = ['/', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "#classes2keep_folder = ['P', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "classes2keep = ['N','/', 'A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "\n",
        "# d_set = \"ECG5000\\\\\"\n",
        "# classes2keep = [1, 2, 3, 4, 5]\n",
        "\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\", d_set, \"multiclass\",\n",
        "                            \"genbeats_ac_wgan_gp_cl_{}_len_ratio_{}\".format(classes2keep_folder, len_ratio_str))\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "NUM_CLASSES = len(classes2keep)\n",
        "IMG_SIZE = 256\n",
        "\n",
        "#with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    #data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = copy.deepcopy(data2keep_dict)\n",
        "data_test_dict = copy.deepcopy(data2keep_dict)\n",
        "\n",
        "for item in data:\n",
        "    if item[2] in classes2keep:\n",
        "        data2keep_dict[item[2]].append(item[1])\n",
        "del data\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "# randomly splitting the dataset into train and test sets\n",
        "for key in classes2keep_folder:\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "# data_train_dict contains all the training data\n",
        "# X_train contains only a portion of training data (len_ratio), to study of size of training set on\n",
        "#           classification metrics\n",
        "\n",
        "a = 0\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train.extend(idx)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jhECFRSfg3N",
        "outputId": "3a0a6a84-8f6a-4e60-f54a-9207f0a20a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**************************************************\n",
            "\t\tstart time:      22:15:11\n",
            "**************************************************\n",
            "train: Counter({0: 67546, 3: 7267, 4: 6533, 1: 6325, 2: 2291, 5: 883, 7: 722, 6: 206, 8: 135})\n",
            "test: Counter({0: 7505, 3: 807, 4: 725, 1: 702, 2: 254, 5: 98, 7: 80, 6: 22, 8: 14})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pywt scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYMV2pSIiIHu",
        "outputId": "08c95539-e243-4cf7-f8a3-3878355f81ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TxszozLamp2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "from sklearn import svm\n"
      ],
      "metadata": {
        "id": "Vcs6mwLQiJhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize feature vector and labels\n",
        "feature_vector = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the beats in the dictionary\n",
        "for cls, beats in data_train_dict.items():\n",
        "    for beat in beats:\n",
        "        # Perform wavelet decomposition at 4 levels\n",
        "        coeffs = pywt.wavedec(beat, 'db6', level=4)\n",
        "\n",
        "        # Extract approximation coefficients at level 4\n",
        "        approx_coeffs = coeffs[0]\n",
        "\n",
        "        # Append approximation coefficients to feature vector\n",
        "        feature_vector.append(approx_coeffs)\n",
        "\n",
        "        # Append label for the beat\n",
        "        labels.append(cls)  # Assuming the record number is the label\n",
        "\n",
        "# Convert feature vector and labels to numpy arrays\n",
        "feature_vector = np.array(feature_vector)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "id": "5tr8dwl-iOnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, train_size=0.8, random_state = 0)"
      ],
      "metadata": {
        "id": "ibqWtq-wnd4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "VxojSip2osb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear_pred = linear.predict(X_test)\n",
        "poly_pred = poly.predict(X_test)\n",
        "#rbf_pred = rbf.predict(X_test)\n",
        "#sig_pred = sig.predict(X_test)"
      ],
      "metadata": {
        "id": "wtQWMVy2ppVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy_lin = linear.score(X_test, y_test)\n",
        "accuracy_poly = poly.score(X_test, y_test)\n",
        "#accuracy_rbf = rbf.score(X_test, y_test)\n",
        "#accuracy_sig = sig.score(X_test, y_test)\n",
        "#print(\"Accuracy Linear Kernel:\", accuracy_lin)\n",
        "print(\"Accuracy Polynomial Kernel:\", accuracy_poly)\n",
        "#print(\"Accuracy Radial Basis Kernel:\", accuracy_rbf)\n",
        "#print(\"Accuracy Sigmoid Kernel:\", accuracy_sig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHyVWxaIpxh5",
        "outputId": "171e26ca-a250-4c5a-b760-f2691f17e2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Polynomial Kernel: 0.9328691110869328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a confusion matrix\n",
        "cm_lin = confusion_matrix(y_test, linear_pred)\n",
        "cm_poly = confusion_matrix(y_test, poly_pred)\n",
        "cm_rbf = confusion_matrix(y_test, rbf_pred)\n",
        "cm_sig = confusion_matrix(y_test, sig_pred)\n",
        "print(cm_lin)\n",
        "print(cm_poly)\n",
        "print(cm_rbf)\n",
        "print(cm_sig)"
      ],
      "metadata": {
        "id": "c0CL36uXrI2e",
        "outputId": "ec4dd9f6-5be9-4f5f-9db7-f55ade53344d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  193     0    10   260    13    20     0     0     0]\n",
            " [    1    43     1    87     1     0     0     0     0]\n",
            " [    2     0   749   689     7     7     0     0     0]\n",
            " [   29    18   110 13226    66    47     1     2     0]\n",
            " [   20     0    11   159  1094     3     0     0     0]\n",
            " [    8     0     0   630     0   660     0     0     0]\n",
            " [    4     0     4    12     0     1     4     0     0]\n",
            " [    0     0     8   148     5     0     0     0     0]\n",
            " [    0     0     1    28     0     0     0     0     0]]\n",
            "[[  241     0    13   239     0     2     1     0     0]\n",
            " [    0    93     1    37     1     1     0     0     0]\n",
            " [    6     0  1330   111     3     3     0     1     0]\n",
            " [   21    11    62 13334    33    36     0     1     1]\n",
            " [    0     0     5    51  1230     1     0     0     0]\n",
            " [    1     0     2   409     2   884     0     0     0]\n",
            " [    2     0     1    11     0     0    11     0     0]\n",
            " [    0     3    16   127     9     0     0     6     0]\n",
            " [    0     0     0    29     0     0     0     0     0]]\n",
            "[[  337     0     0   159     0     0     0     0     0]\n",
            " [    0    79     0    54     0     0     0     0     0]\n",
            " [    0     0  1200   251     0     0     0     3     0]\n",
            " [    2     2     6 13485     4     0     0     0     0]\n",
            " [    0     0     0   169  1115     0     0     3     0]\n",
            " [    0     0     0   121     0  1177     0     0     0]\n",
            " [    0     0     0    25     0     0     0     0     0]\n",
            " [    0     0     7    69     6     0     0    79     0]\n",
            " [    0     0     0    29     0     0     0     0     0]]\n",
            "[[   11     1     0   459    25     0     0     0     0]\n",
            " [    2     0     0   129     2     0     0     0     0]\n",
            " [   89     7     0  1104   247     7     0     0     0]\n",
            " [ 1523    51     3 11721   170    31     0     0     0]\n",
            " [  405     0     0   647   235     0     0     0     0]\n",
            " [   68     1     0  1206    22     1     0     0     0]\n",
            " [    0     0     0    25     0     0     0     0     0]\n",
            " [   26     6     0   124     4     1     0     0     0]\n",
            " [    1     0     0    28     0     0     0     0     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = ['N','P', 'A', 'L','R', 'f', 'j','F','a']  # Replace with your actual class labels\n",
        "\n",
        "# Initialize the confusion matrix\n",
        "num_classes = len(class_labels)\n",
        "confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "# Populate the confusion matrix\n",
        "for true_label, predicted_label in zip(y_test, poly_pred):\n",
        "    confusion_matrix[true_label, predicted_label] += 1\n",
        "\n",
        "# Plot the confusion matrix with labels\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "\n",
        "tick_marks = np.arange(num_classes)\n",
        "plt.xticks(tick_marks, class_labels)\n",
        "plt.yticks(tick_marks, class_labels)\n",
        "\n",
        "thresh = confusion_matrix.max() / 2.0\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oBPGuGNHAmk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming you have trained the `poly` kernel SVM model and stored it in the `poly` variable\n",
        "\n",
        "# Specify the path where you want to save the model\n",
        "model_path = 'dataset/poly_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(poly, model_path)"
      ],
      "metadata": {
        "id": "7QnW7VE-S0Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b16da34-2623-4e33-df6d-656d7a3cdee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset/poly_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming you have trained the `poly` kernel SVM model and stored it in the `poly` variable\n",
        "\n",
        "# Specify the path where you want to save the model\n",
        "model_path = 'dataset/poly_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(poly, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHX6gbxxhIVc",
        "outputId": "eef4328c-9422-467a-8fe7-75f8c7051a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset/polygan_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'dataset/linear_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(linear, model_path)\n",
        "\n",
        "model_path = 'dataset/rbf_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(rbf, model_path)\n",
        "\n",
        "model_path = 'dataset/sig_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(sig, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKc85IYLIP-K",
        "outputId": "8291b302-5332-475e-e7ae-2e73957e8efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset/sig_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOW GAN TRAIN . Training the model on the generated model and testing on the real data.GAN-train is the accuracy of a classifier trained on Sg and tested on a validation set of real images Sv"
      ],
      "metadata": {
        "id": "P5rpHbMWaWRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import csv\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "\n",
        "from ekg_class import dicts\n",
        "from model_ac_wgan_gp_ecg import Disc_ac_wgan_gp_1d, Gen_ac_wgan_gp_1d, initialize_weights_1d\n",
        "from utils import gradient_penalty, normalize, grid_plot_save, grid_plot\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.datetime.now()\n",
        "print((\"\\n\" + \"*\" * 50 + \"\\n\\t\\tstart time:      {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 50).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "#drive = \"F:\"\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "myPath_dataset = os.path.join(myPath_base, \"\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dry_run = False\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 12\n",
        "BATCH_SIZE = 16\n",
        "CHANNELS_IMG = 1\n",
        "CRITIC_ITERATIONS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "GEN_EMBEDDING = 100\n",
        "LAMBDA_GP = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "Z_DIM = 100\n",
        "\n",
        "tr_ts_ratio = 0.1    # \"training set / test set\" split ratio\n",
        "len_ratio = 1     # to study the effect of support set number of samples (shorter train sets)\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', \"\")\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# transforms = transforms.Compose(\n",
        "#    [\n",
        "#        transforms.Resize(IMG_SIZE),\n",
        "#        transforms.ToTensor(),\n",
        "#        transforms.Normalize(\n",
        "#            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "#    ]\n",
        "# )\n",
        "\n",
        "\n",
        "with open(os.path.join(myPath_dataset, \"gb_together.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 1])\n",
        "\n",
        "all_classes = ['/', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "all_classes_folder = ['P', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "\n",
        "all_classes.remove('Q')\n",
        "all_classes.remove('S')\n",
        "# all_classes.remove('V')      # why remove this? plots show no-uniform pattern\n",
        "\n",
        "\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "d_set = \"MIT_BIH\"\n",
        "# Note: '/' and 'P' are the same\n",
        "classes2keep = ['P','A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['P','L','R', 'f', 'j','F','a']\n",
        "\n",
        "# d_set = \"ECG5000\\\\\"\n",
        "# classes2keep = [1, 2, 3, 4, 5]\n",
        "\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\",  \"gan_test_report\")\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "NUM_CLASSES = len(classes2keep)\n",
        "IMG_SIZE = 256\n",
        "\n",
        "#with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    #data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 1])\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = copy.deepcopy(data2keep_dict)\n",
        "data_test_dict = copy.deepcopy(data2keep_dict)\n",
        "count_N=0\n",
        "count_j=0\n",
        "for item in data:\n",
        "    if item[1] in classes2keep:\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "del data\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "for key in classes2keep_folder:\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "\n",
        "X_train_gb = []\n",
        "y_train_gb = []\n",
        "# data_train_dict contains all the training data\n",
        "# X_train contains only a portion of training data (len_ratio), to study of size of training set on\n",
        "#           classification metrics\n",
        "\n",
        "a = 0\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train_gb.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train_gb.extend(idx)\n",
        "\n",
        "X_test_gb = []\n",
        "y_test_gb = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test_gb.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test_gb.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test_gb)\n",
        "y_train_stat = Counter(y_train_gb)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ziVBa58aIc2",
        "outputId": "4c428757-12ce-4912-9f6d-5d97dae850a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**************************************************\n",
            "\t\tstart time:      16:15:51\n",
            "**************************************************\n",
            "train: Counter({0: 1001, 2: 1001, 6: 1001, 1: 1000, 3: 1000, 4: 1000, 7: 1000, 5: 900})\n",
            "test: Counter({0: 9010, 2: 9008, 6: 9008, 3: 9007, 4: 9004, 1: 9001, 7: 8999, 5: 8100})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize feature vector and labels\n",
        "feature_vector = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the beats in the dictionary\n",
        "for cls, beats in data_test_dict.items():\n",
        "    for beat in beats:\n",
        "        # Perform wavelet decomposition at 4 levels\n",
        "        coeffs = pywt.wavedec(beat, 'db6', level=4)\n",
        "\n",
        "        # Extract approximation coefficients at level 4\n",
        "        approx_coeffs = coeffs[0]\n",
        "\n",
        "        # Append approximation coefficients to feature vector\n",
        "        feature_vector.append(approx_coeffs)\n",
        "\n",
        "        # Append label for the beat\n",
        "        labels.append(cls)  # Assuming the record number is the label\n",
        "\n",
        "# Convert feature vector and labels to numpy arrays\n",
        "feature_vector = np.array(feature_vector)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "ia2mZo8bbdPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(feature_vector, labels, train_size=0.8, random_state = 0)"
      ],
      "metadata": {
        "id": "Unx1ANzCb5ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "#rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train_gb, y_train_gb)\n",
        "#sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gK6mJqLWb3kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear_pred = linear.predict(X_test_gb)\n",
        "poly_pred = poly.predict(X_test_gb)\n",
        "#rbf_pred = rbf.predict(X_test_gb)\n",
        "#sig_pred = sig.predict(X_test_gb)"
      ],
      "metadata": {
        "id": "BFBmUoTTcMXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy_lin = linear.score(X_test_gb, y_test_gb)\n",
        "accuracy_poly = poly.score(X_test_gb, y_test_gb)\n",
        "#accuracy_rbf = rbf.score(X_test_gb, y_test_gb)\n",
        "#accuracy_sig = sig.score(X_test_gb, y_test_gb)\n",
        "#print(\"Accuracy Linear Kernel:\", accuracy_lin)\n",
        "print(\"Accuracy Polynomial Kernel:\", accuracy_poly)\n",
        "#print(\"Accuracy Radial Basis Kernel:\", accuracy_rbf)\n",
        "#print(\"Accuracy Sigmoid Kernel:\", accuracy_sig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV9GAx68dNNZ",
        "outputId": "f5abe840-6bb0-4eb6-bbdf-54f490b1d5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Polynomial Kernel: 0.9999297160528535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a confusion matrix\n",
        "#cm_lin = confusion_matrix(y_test_gb, linear_pred)\n",
        "cm_poly = confusion_matrix(y_test_gb, poly_pred)\n",
        "#cm_rbf = confusion_matrix(y_test_gb, rbf_pred)\n",
        "#cm_sig = confusion_matrix(y_test_gb, sig_pred)\n",
        "#print(cm_lin)\n",
        "print(cm_poly)\n",
        "#print(cm_rbf)\n",
        "#print(cm_sig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmWDy325dkc0",
        "outputId": "f9569a68-67fc-4dcc-f4fa-855fd35929e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1832    0    0    0    0    0    0    0]\n",
            " [   0 1757    0    0    1    0    0    0]\n",
            " [   0    0 1757    0    0    0    0    0]\n",
            " [   0    0    0 1775    0    0    0    0]\n",
            " [   0    0    0    0 1819    0    0    0]\n",
            " [   0    0    0    0    0 1779    0    0]\n",
            " [   0    0    0    0    0    0 1868    0]\n",
            " [   0    0    0    0    0    0    0 1640]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test dataset from real data( )\n",
        "\n",
        "#making the test dataset from real data\n",
        "\n",
        "import copy\n",
        "import csv\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "\n",
        "from ekg_class import dicts\n",
        "from model_ac_wgan_gp_ecg import Disc_ac_wgan_gp_1d, Gen_ac_wgan_gp_1d, initialize_weights_1d\n",
        "from utils import gradient_penalty, normalize, grid_plot_save, grid_plot\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.datetime.now()\n",
        "print((\"\\n\" + \"*\" * 50 + \"\\n\\t\\tstart time:      {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 50).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "#drive = \"F:\"\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "myPath_dataset = os.path.join(myPath_base, \"\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dry_run = False\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 12\n",
        "BATCH_SIZE = 16\n",
        "CHANNELS_IMG = 1\n",
        "CRITIC_ITERATIONS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "GEN_EMBEDDING = 100\n",
        "LAMBDA_GP = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "Z_DIM = 100\n",
        "\n",
        "tr_ts_ratio = 0.1     # \"training set / test set\" split ratio\n",
        "len_ratio = 1     # to study the effect of support set number of samples (shorter train sets)\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', \"\")\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# transforms = transforms.Compose(\n",
        "#    [\n",
        "#        transforms.Resize(IMG_SIZE),\n",
        "#        transforms.ToTensor(),\n",
        "#        transforms.Normalize(\n",
        "#            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "#    ]\n",
        "# )\n",
        "\n",
        "\n",
        "with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "all_classes = ['/', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "all_classes_folder = ['P', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "\n",
        "all_classes.remove('Q')\n",
        "all_classes.remove('S')\n",
        "# all_classes.remove('V')      # why remove this? plots show no-uniform pattern\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%    begin classes statistics      %%%%%%%%%%%%%%%%%%\n",
        "\"\"\"\n",
        "stats_all_classes =\n",
        "        {\n",
        "        'N': 75052,   'L': 8075,  'R': 7259,  'V': 7130,\n",
        "        '/': 7028,    'A': 2546,  'f': 982,   'F': 803,\n",
        "        'j': 229,     'a': 150,   'E': 106,   'J': 83,\n",
        "        'Q': 33,      'e': 16,    'S': 2\n",
        "        }\n",
        "P: Paced beat\n",
        "A: Atrial Premature contraction\n",
        "E: Ventricular Escape beat\n",
        "F: Fusion of Ventricular and Normal beat\n",
        "J: Nodal (junctional) Premature Beat\n",
        "L: Left bundle branch block beat\n",
        "N: Normal beat\n",
        "Q: Unclassifiable beat\n",
        "R: Right bundle branch block beat\n",
        "S: Premature or ectopic supraventricular beat\n",
        "V: Premature Ventricular Contraction\n",
        "a: Aberrated Atrial Premature beat\n",
        "e: Atrial escape beat\n",
        "f: Fusion of paced and normal beat\n",
        "j: Nodal (junctional) escape beat\n",
        "\"\"\"\n",
        "# %%%%%%%%%%%%%%%%%    end classes statistics    %%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "d_set = \"MIT_BIH\"\n",
        "# Note: '/' and 'P' are the same\n",
        "#classes2keep = ['/', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "#classes2keep_folder = ['P', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "classes2keep = [ 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['L','R', 'f', 'j','F','a']\n",
        "\n",
        "# d_set = \"ECG5000\\\\\"\n",
        "# classes2keep = [1, 2, 3, 4, 5]\n",
        "\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\", d_set, \"multiclass\",\n",
        "                            \"genbeats_ac_wgan_gp_cl_{}_len_ratio_{}\".format(classes2keep_folder, len_ratio_str))\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "NUM_CLASSES = len(classes2keep)\n",
        "IMG_SIZE = 256\n",
        "\n",
        "#with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    #data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = copy.deepcopy(data2keep_dict)\n",
        "data_test_dict = copy.deepcopy(data2keep_dict)\n",
        "\n",
        "for item in data:\n",
        "    if item[2] in classes2keep:\n",
        "        data2keep_dict[item[2]].append(item[1])\n",
        "del data\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "\n",
        "max_items_per_class = 1400\n",
        "\n",
        "\n",
        "\n",
        "with open(os.path.join(myPath_dataset, \"gb_together.json\"), \"r\") as f:\n",
        "    data_gb = json.load(f)\n",
        "classes2gb=['L','R','f']\n",
        "\n",
        "class_counts = {class_label: 0 for class_label in classes2gb}\n",
        "\n",
        "\n",
        "for item in data_gb:\n",
        "    if item[1] in classes2gb and class_counts[item[1]] < max_items_per_class:\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        class_counts[item[1]] += 1\n",
        "\n",
        "del data_gb\n",
        "\n",
        "\n",
        "# randomly splitting the dataset into train and test sets\n",
        "for key in classes2keep_folder:\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "# data_train_dict contains all the training data\n",
        "# X_train contains only a portion of training data (len_ratio), to study of size of training set on\n",
        "#           classification metrics\n",
        "\n",
        "a = 0\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train.extend(idx)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn7lZ2anfemR",
        "outputId": "52890e02-da4d-48b8-eee3-e013bc2d97a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**************************************************\n",
            "\t\tstart time:      17:16:37\n",
            "**************************************************\n",
            "train: Counter({0: 7505, 1: 807, 2: 725, 3: 98, 5: 80, 4: 22, 6: 15})\n",
            "test: Counter({0: 67546, 1: 7267, 2: 6533, 3: 883, 5: 722, 4: 206, 6: 134})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize feature vector and labels\n",
        "feature_vector = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the beats in the dictionary\n",
        "for cls, beats in data_test_dict.items():\n",
        "    for beat in beats:\n",
        "        # Perform wavelet decomposition at 4 levels\n",
        "        coeffs = pywt.wavedec(beat, 'db6', level=4)\n",
        "\n",
        "        # Extract approximation coefficients at level 4\n",
        "        approx_coeffs = coeffs[0]\n",
        "\n",
        "        # Append approximation coefficients to feature vector\n",
        "        feature_vector.append(approx_coeffs)\n",
        "\n",
        "        # Append label for the beat\n",
        "        labels.append(cls)  # Assuming the record number is the label\n",
        "\n",
        "# Convert feature vector and labels to numpy arrays\n",
        "feature_vector = np.array(feature_vector)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "4pix6-UGh_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, train_size=0.9, random_state = 0)"
      ],
      "metadata": {
        "id": "Ujq1eX09mKMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmdRKQLj5HkH",
        "outputId": "d82787ee-dfc7-4000-a280-25297215442c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: Counter({'L': 7669, 'R': 7045, 'f': 1914, 'F': 650, 'j': 185, 'a': 109})\n",
            "test: Counter({'L': 858, 'R': 748, 'f': 229, 'F': 72, 'a': 25, 'j': 21})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_poly = poly.score(X_test, y_test)\n",
        "#accuracy_rbf = rbf.score(X_test_gb, y_test_gb)\n",
        "#accuracy_sig = sig.score(X_test_gb, y_test_gb)\n",
        "#print(\"Accuracy Linear Kernel:\", accuracy_lin)\n",
        "print(\"Accuracy Polynomial Kernel:\", accuracy_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VaE5IjkmUYM",
        "outputId": "de61b5fc-b13d-409c-9a7d-5534074df49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Polynomial Kernel: 0.7014848950332822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_pred = poly.predict(X_test)\n",
        "cm_poly = confusion_matrix(y_test, poly_pred)\n",
        "#cm_rbf = confusion_matrix(y_test_gb, rbf_pred)\n",
        "#cm_sig = confusion_matrix(y_test_gb, sig_pred)\n",
        "#print(cm_lin)\n",
        "print(cm_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5sowbnkm705",
        "outputId": "a53ed23d-c3d8-4733-c811-a82afa44639b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 507   90   16   26   37    0   15   20]\n",
            " [   2 5697   14 1359  118    0   72  208]\n",
            " [   0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0]\n",
            " [ 345  391  242 1435 3749    0   75  499]\n",
            " [   5   28   18    0   35   20   17   10]\n",
            " [ 125  454   25   47   91    0  387    7]\n",
            " [   1   39   51   21   10    0   18   64]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgh8OMGml3gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wc-tW6aNl3rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now gan-test . ,For getting the GAN-test score, SVM needs to be trained on the original ECG sample and tested on the generated ECG sample. The higher the GAN-test score, the better the fidelity of the generated data . Here we are keeping the test size same as the real dataset that was used for training the template model."
      ],
      "metadata": {
        "id": "2be5g-Yp_pqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Specify the path where the model is saved\n",
        "model_path = 'dataset/poly_model.pkl'\n",
        "\n",
        "# Load the saved model\n",
        "poly= joblib.load(model_path)\n",
        "\n",
        "model_path = 'dataset/linear_model.pkl'\n",
        "\n",
        "# Load the saved model\n",
        "linear= joblib.load(model_path)\n",
        "\n",
        "model_path = 'dataset/rbf_model.pkl'\n",
        "\n",
        "# Load the saved model\n",
        "rbf= joblib.load(model_path)\n",
        "\n",
        "model_path = 'dataset/sig_model.pkl'\n",
        "\n",
        "# Load the saved model\n",
        "sig= joblib.load(model_path)"
      ],
      "metadata": {
        "id": "hj31nsWll3za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import csv\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "\n",
        "from ekg_class import dicts\n",
        "from model_ac_wgan_gp_ecg import Disc_ac_wgan_gp_1d, Gen_ac_wgan_gp_1d, initialize_weights_1d\n",
        "from utils import gradient_penalty, normalize, grid_plot_save, grid_plot\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.datetime.now()\n",
        "print((\"\\n\" + \"*\" * 50 + \"\\n\\t\\tstart time:      {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 50).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "#drive = \"F:\"\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "myPath_dataset = os.path.join(myPath_base, \"\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dry_run = False\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 12\n",
        "BATCH_SIZE = 16\n",
        "CHANNELS_IMG = 1\n",
        "CRITIC_ITERATIONS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "GEN_EMBEDDING = 100\n",
        "LAMBDA_GP = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "Z_DIM = 100\n",
        "\n",
        "tr_ts_ratio = 0.0    # \"training set / test set\" split ratio\n",
        "len_ratio = 1     # to study the effect of support set number of samples (shorter train sets)\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', \"\")\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# transforms = transforms.Compose(\n",
        "#    [\n",
        "#        transforms.Resize(IMG_SIZE),\n",
        "#        transforms.ToTensor(),\n",
        "#        transforms.Normalize(\n",
        "#            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "#    ]\n",
        "# )\n",
        "\n",
        "\n",
        "with open(os.path.join(myPath_dataset, \"gb_together.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 1])\n",
        "\n",
        "all_classes = ['/', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "all_classes_folder = ['P', 'A', 'E', 'F', 'J', 'L', 'N', 'Q', 'R', 'S', 'V', 'a', 'e', 'f', 'j']\n",
        "\n",
        "all_classes.remove('Q')\n",
        "all_classes.remove('S')\n",
        "# all_classes.remove('V')      # why remove this? plots show no-uniform pattern\n",
        "\n",
        "\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "d_set = \"MIT_BIH\"\n",
        "# Note: '/' and 'P' are the same\n",
        "classes2keep = ['N','P','A' ,'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P','A','L','R', 'f', 'j','F','a']\n",
        "\n",
        "# d_set = \"ECG5000\\\\\"\n",
        "# classes2keep = [1, 2, 3, 4, 5]\n",
        "\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\",  \"gan_test_report\")\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "NUM_CLASSES = len(classes2keep)\n",
        "IMG_SIZE = 256\n",
        "\n",
        "#with open(os.path.join(myPath_dataset, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    #data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 1])\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = copy.deepcopy(data2keep_dict)\n",
        "data_test_dict = copy.deepcopy(data2keep_dict)\n",
        "count_f=0\n",
        "count_j=0\n",
        "count_F=0\n",
        "count_a=0\n",
        "count_P=0\n",
        "count_A=0\n",
        "count_L=0\n",
        "count_R=0\n",
        "count_N=0\n",
        "for item in data:\n",
        "    if item[1] == 'f'  and count_f< 99:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_f += 1\n",
        "    if item[1] == 'j'  and count_j < 23:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_j += 1\n",
        "    if item[1] == 'F'  and count_F < 81:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_F += 1\n",
        "\n",
        "    if item[1] == 'a'  and count_a < 15:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_a += 1\n",
        "\n",
        "    if item[1] == 'A'  and count_A< 255:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_A += 1\n",
        "\n",
        "    if item[1] == 'P'  and count_P< 702:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_P += 1\n",
        "    if item[1] == 'R'  and count_R< 726:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_R += 1\n",
        "    if item[1] == 'N'  and count_N< 7506:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_N += 1\n",
        "    elif item[1] == 'L'  and count_L< 808:  # Add condition to limit 'N' data to 4000\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        count_L += 1\n",
        "\n",
        "del data\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "for key in classes2keep_folder:\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "\n",
        "X_train_gb = []\n",
        "y_train_gb = []\n",
        "# data_train_dict contains all the training data\n",
        "# X_train contains only a portion of training data (len_ratio), to study of size of training set on\n",
        "#           classification metrics\n",
        "\n",
        "a = 0\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train_gb.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train_gb.extend(idx)\n",
        "\n",
        "X_test_gb = []\n",
        "y_test_gb = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test_gb.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test_gb.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test_gb)\n",
        "y_train_stat = Counter(y_train_gb)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN38e468l3-t",
        "outputId": "fe4811d4-41dd-49d6-fadc-5532d9ea9729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**************************************************\n",
            "\t\tstart time:      17:03:36\n",
            "**************************************************\n",
            "train: Counter()\n",
            "test: Counter({0: 7505, 3: 807, 4: 725, 1: 701, 2: 254, 5: 98, 7: 80, 6: 22, 8: 14})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize feature vector and labels\n",
        "feature_vector = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the beats in the dictionary\n",
        "for cls, beats in data_test_dict.items():\n",
        "    for beat in beats:\n",
        "        # Perform wavelet decomposition at 4 levels\n",
        "        coeffs = pywt.wavedec(beat, 'db6', level=4)\n",
        "\n",
        "        # Extract approximation coefficients at level 4\n",
        "        approx_coeffs = coeffs[0]\n",
        "\n",
        "        # Append approximation coefficients to feature vector\n",
        "        feature_vector.append(approx_coeffs)\n",
        "\n",
        "        # Append label for the beat\n",
        "        labels.append(cls)  # Assuming the record number is the label\n",
        "\n",
        "# Convert feature vector and labels to numpy arrays\n",
        "feature_vector = np.array(feature_vector)\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(feature_vector, labels, train_size=1, random_state = 0)"
      ],
      "metadata": {
        "id": "OBSAFzj8mZNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_stat = Counter(y_test_gb)\n",
        "y_train_stat = Counter(y_train_gb)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))\n",
        "\n",
        "\n",
        "poly_pred = poly.predict(X_test_gb)\n",
        "\n",
        "\n",
        "accuracy_poly = poly.score(X_test_gb, y_test_gb)\n",
        "\n",
        "print(\"Accuracy Polynomial Kernel:\", accuracy_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pq5cWCgmZeP",
        "outputId": "1fb92322-aa50-4ff4-a010-189c2e990ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: Counter({'N': 1})\n",
            "test: Counter({'N': 7504, 'L': 807, 'R': 725, 'P': 701, 'A': 254, 'f': 98, 'F': 80, 'j': 22, 'a': 14})\n",
            "Accuracy Polynomial Kernel: 0.9304262616364527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_poly = confusion_matrix(y_test_gb, poly_pred)\n",
        "#cm_rbf = confusion_matrix(y_test_gb, rbf_pred)\n",
        "#cm_sig = confusion_matrix(y_test_gb, sig_pred)\n",
        "#print(cm_lin)\n",
        "print(cm_poly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mhs4NS6x_15",
        "outputId": "70fef355-6e2e-4865-98c3-ca317cddca87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  19    0    0  235    0    0    0    0    0]\n",
            " [   0   78    0    2    0    0    0    0    0]\n",
            " [   0    1  750   55    1    0    0    0    0]\n",
            " [   0    0    1 7503    0    0    0    0    0]\n",
            " [   0    0    0    8  693    0    0    0    0]\n",
            " [   0    0    0  275    0  450    0    0    0]\n",
            " [   0    0    0   12    0    0    2    0    0]\n",
            " [   0    0    2   96    0    0    0    0    0]\n",
            " [   0    0    0   22    0    0    0    0    0]]\n"
          ]
        }
      ]
    }
  ]
}