{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtjyORbEBOe+XCXyuDBbBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greymattar/ECG_WGAN_AugmentationClassification/blob/main/50_Aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXe6TScYBkD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b10bd7-6011-4951-d4ff-8a99e481e54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/gdrive')\n",
        "import sys\n",
        "import os\n",
        "#sys.path.append('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "os.chdir('/content/gdrive/MyDrive/Augmentation-of-ECG-Training-Dataset-with-CGAN-main')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B47xLmn2llcT",
        "outputId": "5b556a03-511e-43dc-b952-5e8e4996f440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uv-Z8hEVllvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into training and testing\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import csv\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from ekg_class import dicts\n",
        "import torch.nn as nn\n",
        "from models_classifier import EcgResNet34\n",
        "from sklearn.metrics import classification_report as report\n",
        "from sklearn.metrics import confusion_matrix as cf_matrix\n",
        "from utils import print_confusion_matrix\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.now()\n",
        "\n",
        "print((\"\\n\" + \"*\" * 61 + \"\\n\\t\\t\\t\\t\\tstart time  {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 61).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "path_aux = 'paper3_DM\\\\paper3_data'\n",
        "myPath_base = os.path.join(myPath_base, '')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyper-parameters etc.\n",
        "dry_run = False\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_CLASSES = 9\n",
        "tr_ts_ratio = 0.9     # \"training set / test set\" split ratio\n",
        "len_ratio = 0.5\n",
        "\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 10\n",
        "#classes2keep = ['/', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "#classes2keep_folder = ['P', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "\n",
        "classes2keep = ['N','/', 'A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "\n",
        "\n",
        "#key_aug = \"aug\"\n",
        "key_aug = \"notaug\";\n",
        "\n",
        "#key_bal = 'balanced'\n",
        "key_bal = 'imbalanced';\n",
        "\n",
        "# key_case = 'rl'\n",
        "# key_case = \"02\"\n",
        "key_case = \"wgan\"\n",
        "\n",
        "len_ratio = 0.5                   # shorter train sets\n",
        "#len_ratio = 1                  # shorter train sets\n",
        "num_samples = 8000\n",
        "tst_len = 1000\n",
        "print('\\ncase: {}, {}, {}\\n'.format(key_case, key_bal, key_aug))\n",
        "# num_N_samples = int(len_ratio*num_samples)\n",
        "\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', '')\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "path = \"gb_dm_case_{}\".format(key_case)\n",
        "myPath_save = os.path.join(myPath_base, path)\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "brk = 'here'\n",
        "\n",
        "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   begin: From whole database select training and testing set   %%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "d_set = \"MIT_BIH/tesing\"\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\", d_set,\"50classifier_wgan_gp_cl_{}_{}\".format(classes2keep_folder,len_ratio))\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "with open(os.path.join(myPath_base, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = copy.deepcopy(data2keep_dict)\n",
        "data_test_dict = copy.deepcopy(data2keep_dict)\n",
        "\n",
        "for item in data:\n",
        "    if item[2] in classes2keep:\n",
        "        data2keep_dict[item[2]].append(item[1])\n",
        "del data\n",
        "\n",
        "# randomly splitting the dataset into train and test sets\n",
        "for key in data2keep_dict.keys():\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "    temp = data_train_dict['/']\n",
        "    data_train_dict.pop('/')\n",
        "    data_train_dict['P'] = temp\n",
        "\n",
        "    temp = data_test_dict['/']\n",
        "    data_test_dict.pop('/')\n",
        "    data_test_dict['P'] = temp\n",
        "\n",
        "\n",
        "# create X, y for train and test sets\n",
        "X_train = []\n",
        "y_train = []\n",
        "'''\n",
        "for key in data_train_dict.keys():\n",
        "    X_train.extend(data_train_dict[key])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_train_dict[key])\n",
        "    y_train.extend(idx)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for key in data_test_dict.keys():\n",
        "    X_test.extend(data_test_dict[key])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))\n",
        "'''\n",
        "\n",
        "#for shorter training set\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train.extend(idx)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))\n",
        "a = 0\n",
        "\n",
        "with open(os.path.join(myPath_save, \"X_train.json\"), \"w\") as f:\n",
        "    json.dump(X_train, f)\n",
        "with open(os.path.join(myPath_save, \"y_train.json\"), \"w\") as f:\n",
        "    json.dump(y_train, f)\n",
        "with open(os.path.join(myPath_save, \"X_test.json\"), \"w\") as f:\n",
        "    json.dump(X_test, f)\n",
        "with open(os.path.join(myPath_save, \"y_test.json\"), \"w\") as f:\n",
        "    json.dump(y_test, f)\n",
        "\n",
        "'''\n",
        "path = os.path.join(myPath_base, \"X_N_trn.json\")\n",
        "with open(path, 'r') as f:\n",
        "    X_N_trn = json.load(f)\n",
        "\n",
        "path = os.path.join(myPath_base, \"X_N_tst.json\")\n",
        "with open(path, 'r') as f:\n",
        "    X_N_tst = json.load(f)\n",
        "\n",
        "path = os.path.join(myPath_base, \"X_L_trn.json\")\n",
        "with open(path, 'r') as f:\n",
        "    X_L_trn = json.load(f)\n",
        "\n",
        "path = os.path.join(myPath_base, \"X_L_tst.json\")\n",
        "with open(path, 'r') as f:\n",
        "    X_L_tst = json.load(f)\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  end: load X and y   %%%%%%%%%%%%%%%%%%\n",
        "X_tst = X_N_tst + X_L_tst\n",
        "y_N_tst = [0 for _ in range(len(X_N_tst))]\n",
        "y_L_tst = [1 for _ in range(len(X_L_tst))]\n",
        "y_tst = y_N_tst + y_L_tst\n",
        "\n",
        "if key_aug == 'aug':\n",
        "    with open(os.path.join(myPath_base, 'gb_dm_case_{}\\\\gb_dm_1d_case_{}.json'.format(key_case, key_case)), \"r\") as f:\n",
        "        gb_for_aug = json.load(f)\n",
        "\n",
        "    rand_idx = random.sample(range(0, len(gb_for_aug)), len(gb_for_aug))\n",
        "    temp = [gb_for_aug[idx] for idx in rand_idx]\n",
        "    X_aug = temp[:7000-350]\n",
        "else:\n",
        "    X_aug = []\n",
        "\n",
        "# # imbalanced case N: 350, select 350 elements in X_N_trn if 'imbalanced' or 'augmented'\n",
        "if key_bal == 'imbalanced' or key_aug == 'aug':\n",
        "    rand_idx = random.sample(range(0, len(X_N_trn)), len(X_N_trn))[:350]\n",
        "    X_N_trn = [X_N_trn[idx] for idx in rand_idx]\n",
        "\n",
        "X_N_trn = X_N_trn + X_aug\n",
        "X_trn = X_N_trn + X_L_trn\n",
        "\n",
        "y_N_trn = [0 for _ in range(len(X_N_trn))]\n",
        "y_L_trn = [1 for _ in range(len(X_L_trn))]\n",
        "y_trn = y_N_trn + y_L_trn\n",
        "'''\n",
        "trn_set = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "tst_set = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "trn_loader = DataLoader(trn_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "tst_loader = DataLoader(tst_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "brk = 'here'\n",
        "# %%%%%%%%%%%%%%%%%     begin save sample plots of classes in classes2keep    %%%%%%%%%%%%%%%%%%%%\n",
        "'''\n",
        "for cl in classes2keep:\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=3)\n",
        "    fig.suptitle(\"Class {} ({}: {}), count: {}\".\n",
        "                 format(classes2keep.index(cl), cl, letter2descr[cl], len(data2keep_dict[cl])))\n",
        "\n",
        "    count = 0\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            count += 1\n",
        "            if count >= len(data2keep_dict[cl]):\n",
        "                continue\n",
        "            axes[i][j].plot(data2keep_dict[cl][count])\n",
        "            axes[i][j].grid()\n",
        "    plt.savefig(os.path.join(myPath_save, \"00_sample_cl_{}.png\".format(classes2keep.index(cl))))\n",
        "\n",
        "plt.close(\"all\")\n",
        "'''\n",
        "# %%%%%%%%%%%%%%%%%     end save sample plots of classes in classes2keep      %%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%% begin:     Select and Initialize Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "# net = net_cnn(num_classes=NUM_CLASSES).to(device)\n",
        "# net = net_fc(input_size=INPUT_SIZE, num_classes=NUM_CLASSES).to(device)\n",
        "net = EcgResNet34(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# %%%%%%%%%%%%%%%%%%%% end:     Select and Initialize Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "# %%%%%%%%%%%%%%%%%%%%    begin:  Train Classifier       %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_idx, (inputs, labels) in enumerate(trn_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.reshape(inputs.shape[0], 1, -1)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        scores = net(inputs)\n",
        "        loss = criterion(scores.squeeze(), labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        # running_loss += loss.item()\n",
        "        if batch_idx % 200 == 0:  # print every 200 mini-batches\n",
        "            now = datetime.now()\n",
        "            print('{:02d}:{:02d}:{:02d}\\t\\tepoch={:4d} / {:4d}\\t\\titer={:5d} / {:5d}\\t\\t\\tloss: {:7.5f}'.\n",
        "                  format(now.hour, now.minute, now.second, epoch, NUM_EPOCHS, batch_idx, len(trn_loader), loss))\n",
        "\n",
        "print('\\n\\tFinished Training\\n')\n",
        "# %%%%%%%%%%%%%%%%%%%%    end  Train Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "print('\\tSaving model ...\\n')\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%    begin save model   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "f_name = \"classifier_{}_{}_{}.pth\".format(key_case, key_bal, key_aug)\n",
        "PATH = os.path.join(myPath_save, f_name)\n",
        "torch.save(net.state_dict(), PATH)\n",
        "# %%%%%%%%%%%%%%%%%%%%    end save model     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAOoz3Ovk0qg",
        "outputId": "a851d3c9-65fe-4aef-8717-bdffd7a4a578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************************************************\n",
            "\t\t\t\t\tstart time  02:08:32\n",
            "*************************************************************\n",
            "\n",
            "case: wgan, imbalanced, notaug\n",
            "\n",
            "train: Counter({0: 33773, 3: 3633, 4: 3266, 1: 3162, 2: 1145, 5: 441, 7: 361, 6: 103, 8: 67})\n",
            "test: Counter({0: 7505, 3: 807, 4: 725, 1: 702, 2: 254, 5: 98, 7: 80, 6: 22, 8: 14})\n",
            "02:09:24\t\tepoch=   0 /   10\t\titer=    0 /  2871\t\t\tloss: 1.19965\n",
            "02:09:48\t\tepoch=   0 /   10\t\titer=  200 /  2871\t\t\tloss: 3.27782\n",
            "02:10:11\t\tepoch=   0 /   10\t\titer=  400 /  2871\t\t\tloss: 2.37190\n",
            "02:10:35\t\tepoch=   0 /   10\t\titer=  600 /  2871\t\t\tloss: 1.25582\n",
            "02:10:59\t\tepoch=   0 /   10\t\titer=  800 /  2871\t\t\tloss: 0.12806\n",
            "02:11:23\t\tepoch=   0 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.80305\n",
            "02:11:48\t\tepoch=   0 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.97326\n",
            "02:12:12\t\tepoch=   0 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.88709\n",
            "02:12:36\t\tepoch=   0 /   10\t\titer= 1600 /  2871\t\t\tloss: 1.01079\n",
            "02:13:00\t\tepoch=   0 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.34382\n",
            "02:13:25\t\tepoch=   0 /   10\t\titer= 2000 /  2871\t\t\tloss: 1.51564\n",
            "02:13:49\t\tepoch=   0 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.62418\n",
            "02:14:13\t\tepoch=   0 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.55051\n",
            "02:14:37\t\tepoch=   0 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.74462\n",
            "02:15:02\t\tepoch=   0 /   10\t\titer= 2800 /  2871\t\t\tloss: 1.13933\n",
            "02:15:10\t\tepoch=   1 /   10\t\titer=    0 /  2871\t\t\tloss: 2.01009\n",
            "02:15:35\t\tepoch=   1 /   10\t\titer=  200 /  2871\t\t\tloss: 0.52525\n",
            "02:15:59\t\tepoch=   1 /   10\t\titer=  400 /  2871\t\t\tloss: 0.89511\n",
            "02:16:23\t\tepoch=   1 /   10\t\titer=  600 /  2871\t\t\tloss: 0.39298\n",
            "02:16:48\t\tepoch=   1 /   10\t\titer=  800 /  2871\t\t\tloss: 0.63206\n",
            "02:17:12\t\tepoch=   1 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.50600\n",
            "02:17:36\t\tepoch=   1 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.90382\n",
            "02:18:00\t\tepoch=   1 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.11693\n",
            "02:18:25\t\tepoch=   1 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.52540\n",
            "02:18:49\t\tepoch=   1 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.03995\n",
            "02:19:14\t\tepoch=   1 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.19611\n",
            "02:19:38\t\tepoch=   1 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.68166\n",
            "02:20:02\t\tepoch=   1 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.04471\n",
            "02:20:26\t\tepoch=   1 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.95857\n",
            "02:20:51\t\tepoch=   1 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.26020\n",
            "02:20:59\t\tepoch=   2 /   10\t\titer=    0 /  2871\t\t\tloss: 0.72269\n",
            "02:21:23\t\tepoch=   2 /   10\t\titer=  200 /  2871\t\t\tloss: 0.75765\n",
            "02:21:48\t\tepoch=   2 /   10\t\titer=  400 /  2871\t\t\tloss: 0.63450\n",
            "02:22:12\t\tepoch=   2 /   10\t\titer=  600 /  2871\t\t\tloss: 0.02714\n",
            "02:22:36\t\tepoch=   2 /   10\t\titer=  800 /  2871\t\t\tloss: 0.44581\n",
            "02:23:01\t\tepoch=   2 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.51084\n",
            "02:23:26\t\tepoch=   2 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.40669\n",
            "02:23:50\t\tepoch=   2 /   10\t\titer= 1400 /  2871\t\t\tloss: 1.01445\n",
            "02:24:14\t\tepoch=   2 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.32825\n",
            "02:24:38\t\tepoch=   2 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.37777\n",
            "02:25:02\t\tepoch=   2 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.03595\n",
            "02:25:26\t\tepoch=   2 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.04128\n",
            "02:25:51\t\tepoch=   2 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.55760\n",
            "02:26:15\t\tepoch=   2 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.43090\n",
            "02:26:39\t\tepoch=   2 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.07745\n",
            "02:26:48\t\tepoch=   3 /   10\t\titer=    0 /  2871\t\t\tloss: 0.00914\n",
            "02:27:12\t\tepoch=   3 /   10\t\titer=  200 /  2871\t\t\tloss: 0.63517\n",
            "02:27:36\t\tepoch=   3 /   10\t\titer=  400 /  2871\t\t\tloss: 0.05356\n",
            "02:28:00\t\tepoch=   3 /   10\t\titer=  600 /  2871\t\t\tloss: 0.26538\n",
            "02:28:25\t\tepoch=   3 /   10\t\titer=  800 /  2871\t\t\tloss: 0.23457\n",
            "02:28:49\t\tepoch=   3 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.10146\n",
            "02:29:13\t\tepoch=   3 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.08995\n",
            "02:29:37\t\tepoch=   3 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.22486\n",
            "02:30:01\t\tepoch=   3 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.13407\n",
            "02:30:25\t\tepoch=   3 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.42524\n",
            "02:30:49\t\tepoch=   3 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.08256\n",
            "02:31:14\t\tepoch=   3 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.07178\n",
            "02:31:38\t\tepoch=   3 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.09354\n",
            "02:32:02\t\tepoch=   3 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.05129\n",
            "02:32:26\t\tepoch=   3 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.58933\n",
            "02:32:34\t\tepoch=   4 /   10\t\titer=    0 /  2871\t\t\tloss: 0.53478\n",
            "02:32:58\t\tepoch=   4 /   10\t\titer=  200 /  2871\t\t\tloss: 0.16152\n",
            "02:33:22\t\tepoch=   4 /   10\t\titer=  400 /  2871\t\t\tloss: 0.02201\n",
            "02:33:46\t\tepoch=   4 /   10\t\titer=  600 /  2871\t\t\tloss: 0.18978\n",
            "02:34:11\t\tepoch=   4 /   10\t\titer=  800 /  2871\t\t\tloss: 0.11427\n",
            "02:34:35\t\tepoch=   4 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.11038\n",
            "02:34:59\t\tepoch=   4 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.60206\n",
            "02:35:23\t\tepoch=   4 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.08645\n",
            "02:35:47\t\tepoch=   4 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.17784\n",
            "02:36:11\t\tepoch=   4 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.05048\n",
            "02:36:35\t\tepoch=   4 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.03019\n",
            "02:36:59\t\tepoch=   4 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.02888\n",
            "02:37:23\t\tepoch=   4 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.15164\n",
            "02:37:47\t\tepoch=   4 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.01299\n",
            "02:38:12\t\tepoch=   4 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.08903\n",
            "02:38:21\t\tepoch=   5 /   10\t\titer=    0 /  2871\t\t\tloss: 0.23612\n",
            "02:38:45\t\tepoch=   5 /   10\t\titer=  200 /  2871\t\t\tloss: 0.01341\n",
            "02:39:09\t\tepoch=   5 /   10\t\titer=  400 /  2871\t\t\tloss: 0.21621\n",
            "02:39:33\t\tepoch=   5 /   10\t\titer=  600 /  2871\t\t\tloss: 0.78918\n",
            "02:39:57\t\tepoch=   5 /   10\t\titer=  800 /  2871\t\t\tloss: 0.18122\n",
            "02:40:21\t\tepoch=   5 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.04417\n",
            "02:40:45\t\tepoch=   5 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.29992\n",
            "02:41:09\t\tepoch=   5 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.51149\n",
            "02:41:33\t\tepoch=   5 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.37502\n",
            "02:41:58\t\tepoch=   5 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.76175\n",
            "02:42:22\t\tepoch=   5 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.10777\n",
            "02:42:46\t\tepoch=   5 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.09922\n",
            "02:43:10\t\tepoch=   5 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.11441\n",
            "02:43:34\t\tepoch=   5 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.42331\n",
            "02:43:58\t\tepoch=   5 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.45965\n",
            "02:44:07\t\tepoch=   6 /   10\t\titer=    0 /  2871\t\t\tloss: 0.14445\n",
            "02:44:31\t\tepoch=   6 /   10\t\titer=  200 /  2871\t\t\tloss: 0.30121\n",
            "02:44:55\t\tepoch=   6 /   10\t\titer=  400 /  2871\t\t\tloss: 0.08830\n",
            "02:45:19\t\tepoch=   6 /   10\t\titer=  600 /  2871\t\t\tloss: 0.13001\n",
            "02:45:43\t\tepoch=   6 /   10\t\titer=  800 /  2871\t\t\tloss: 0.02780\n",
            "02:46:08\t\tepoch=   6 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.03471\n",
            "02:46:32\t\tepoch=   6 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.02040\n",
            "02:46:56\t\tepoch=   6 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.30998\n",
            "02:47:20\t\tepoch=   6 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.04805\n",
            "02:47:44\t\tepoch=   6 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.33487\n",
            "02:48:08\t\tepoch=   6 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.38393\n",
            "02:48:32\t\tepoch=   6 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.04427\n",
            "02:48:56\t\tepoch=   6 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.00382\n",
            "02:49:20\t\tepoch=   6 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.00223\n",
            "02:49:44\t\tepoch=   6 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.01634\n",
            "02:49:53\t\tepoch=   7 /   10\t\titer=    0 /  2871\t\t\tloss: 0.00606\n",
            "02:50:17\t\tepoch=   7 /   10\t\titer=  200 /  2871\t\t\tloss: 0.24125\n",
            "02:50:41\t\tepoch=   7 /   10\t\titer=  400 /  2871\t\t\tloss: 0.01111\n",
            "02:51:05\t\tepoch=   7 /   10\t\titer=  600 /  2871\t\t\tloss: 0.01804\n",
            "02:51:29\t\tepoch=   7 /   10\t\titer=  800 /  2871\t\t\tloss: 0.01831\n",
            "02:51:53\t\tepoch=   7 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.18498\n",
            "02:52:17\t\tepoch=   7 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.02159\n",
            "02:52:41\t\tepoch=   7 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.00784\n",
            "02:53:05\t\tepoch=   7 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.00222\n",
            "02:53:29\t\tepoch=   7 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.04107\n",
            "02:53:53\t\tepoch=   7 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.05268\n",
            "02:54:17\t\tepoch=   7 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.11934\n",
            "02:54:41\t\tepoch=   7 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.02160\n",
            "02:55:05\t\tepoch=   7 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.11960\n",
            "02:55:29\t\tepoch=   7 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.00163\n",
            "02:55:38\t\tepoch=   8 /   10\t\titer=    0 /  2871\t\t\tloss: 0.01612\n",
            "02:56:02\t\tepoch=   8 /   10\t\titer=  200 /  2871\t\t\tloss: 0.05276\n",
            "02:56:26\t\tepoch=   8 /   10\t\titer=  400 /  2871\t\t\tloss: 0.04445\n",
            "02:56:49\t\tepoch=   8 /   10\t\titer=  600 /  2871\t\t\tloss: 0.44781\n",
            "02:57:14\t\tepoch=   8 /   10\t\titer=  800 /  2871\t\t\tloss: 0.00288\n",
            "02:57:38\t\tepoch=   8 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.04220\n",
            "02:58:02\t\tepoch=   8 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.00809\n",
            "02:58:26\t\tepoch=   8 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.02363\n",
            "02:58:50\t\tepoch=   8 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.00907\n",
            "02:59:13\t\tepoch=   8 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.00280\n",
            "02:59:37\t\tepoch=   8 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.35398\n",
            "03:00:01\t\tepoch=   8 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.15521\n",
            "03:00:26\t\tepoch=   8 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.00520\n",
            "03:00:50\t\tepoch=   8 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.03331\n",
            "03:01:14\t\tepoch=   8 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.04126\n",
            "03:01:22\t\tepoch=   9 /   10\t\titer=    0 /  2871\t\t\tloss: 0.05397\n",
            "03:01:46\t\tepoch=   9 /   10\t\titer=  200 /  2871\t\t\tloss: 0.03840\n",
            "03:02:10\t\tepoch=   9 /   10\t\titer=  400 /  2871\t\t\tloss: 0.04949\n",
            "03:02:34\t\tepoch=   9 /   10\t\titer=  600 /  2871\t\t\tloss: 0.02164\n",
            "03:02:58\t\tepoch=   9 /   10\t\titer=  800 /  2871\t\t\tloss: 0.05466\n",
            "03:03:22\t\tepoch=   9 /   10\t\titer= 1000 /  2871\t\t\tloss: 0.00055\n",
            "03:03:46\t\tepoch=   9 /   10\t\titer= 1200 /  2871\t\t\tloss: 0.23059\n",
            "03:04:10\t\tepoch=   9 /   10\t\titer= 1400 /  2871\t\t\tloss: 0.00942\n",
            "03:04:34\t\tepoch=   9 /   10\t\titer= 1600 /  2871\t\t\tloss: 0.02711\n",
            "03:04:58\t\tepoch=   9 /   10\t\titer= 1800 /  2871\t\t\tloss: 0.02014\n",
            "03:05:22\t\tepoch=   9 /   10\t\titer= 2000 /  2871\t\t\tloss: 0.07286\n",
            "03:05:46\t\tepoch=   9 /   10\t\titer= 2200 /  2871\t\t\tloss: 0.02387\n",
            "03:06:10\t\tepoch=   9 /   10\t\titer= 2400 /  2871\t\t\tloss: 0.01246\n",
            "03:06:34\t\tepoch=   9 /   10\t\titer= 2600 /  2871\t\t\tloss: 0.08245\n",
            "03:06:58\t\tepoch=   9 /   10\t\titer= 2800 /  2871\t\t\tloss: 0.67354\n",
            "\n",
            "\tFinished Training\n",
            "\n",
            "\tSaving model ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%%%%%%%%%%%%%%%%%%%    load trained classifier     %%%%%%%%%%%%%%%%%%%%%\n",
        "path = \"gb_dm_case_{}\".format(key_case)\n",
        "myPath = os.path.join(myPath_base, path)\n",
        "classifier_name = \"classifier_{}_{}_{}.pth\".format(key_case, key_bal, key_aug)\n",
        "net.load_state_dict(torch.load(os.path.join(myPath_save, classifier_name)))\n",
        "\n",
        "brk = 'here'\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%    print Classification Report to file      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "trntst = ['tst']\n",
        "for key_trntst in trntst:\n",
        "    print('Preparing Classification Report: {}'.format(key_trntst))\n",
        "    path = os.path.join(myPath_save, 'classification_report_{}_{}_{}_{}.txt'.\n",
        "                        format(key_trntst, key_case, key_bal, key_aug))\n",
        "    with open(path, 'w') as sys.stdout:\n",
        "        y_scores = []\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        exec('loader = {}_loader'.format(key_trntst))\n",
        "        for (inputs, labels) in tqdm(loader):\n",
        "            inputs = inputs.reshape(inputs.shape[0], 1, -1).to(device)\n",
        "            labels = labels.to(device)\n",
        "            temp = net(inputs)\n",
        "            output = temp.max(dim=1)\n",
        "\n",
        "            y_true.extend(labels.data.tolist())\n",
        "            y_pred.extend(output.indices.tolist())\n",
        "            y_scores.extend((F.softmax(temp, dim=1).tolist()))\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"%\" * 20 + \"\\tClassification Report ({} Set, {}, {}, {})\\t\".\n",
        "              format(key_trntst, key_case, key_bal, key_aug) + \"%\" * 20)\n",
        "        print(\"Classes:                             {}\".format(classes2keep_folder))\n",
        "        print(\"Classifier Model:                    {}\".format(net._get_name()))\n",
        "        print(f\"number of epochs:                    {NUM_EPOCHS}\\n\")\n",
        "        print(\"Train set length:                    {}\".format(len(trn_set)))\n",
        "        print(\"Test set length:                     {}\".format(len(tst_set)))\n",
        "        print(\"Train set reduction length ratio:    {}\\t\".format(len_ratio_str))\n",
        "        print('\\nClassification Report:')\n",
        "        print(report(y_true, y_pred, target_names=classes2keep_folder))\n",
        "        print(\"\\nConfusion Matrix:\\n {}\".format(cf_matrix(y_true, y_pred)))\n",
        "        \"\"\"\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true, np.asarray(y_scores)[:, 1])\n",
        "        #here auc function won't work as it works for binary classifier\n",
        "        pr_recall_auc_score = auc(recall, precision)\n",
        "        print('\\nPrecision-Recall AUC Score:        {:6.4f}'.format(pr_recall_auc_score))\n",
        "        rocauc_score = roc_auc_score(y_true, np.asarray(y_scores)[:, 1])\n",
        "        print('\\nROC AUC Score:                     {:6.4f}'.format(rocauc_score))\n",
        "\n",
        "        print_confusion_matrix(cf_matrix(y_true, y_pred), class_names=classes2keep_folder,\n",
        "                               fig_name=\"Conf. Matrix_{}_{}_{}_{}\".format(key_trntst, key_case, key_bal, key_aug))\n",
        "        plt.savefig(os.path.join(myPath_save, \"cfmx_{}_{}_{}_{}.png\".format(key_trntst, key_case, key_bal, key_aug)))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "\n",
        "        plt.plot(precision, recall)\n",
        "        plt.title(\"Precision - Recall Curve ({}, {}, {}, {})\".format(key_trntst, key_case, key_bal, key_aug))\n",
        "        plt.grid()\n",
        "        plt.savefig(os.path.join(myPath_save, \"prec_recall_curve_{}_{}_{}_{}.png\".\n",
        "                                 format(key_trntst, key_case, key_bal, key_aug)))\n",
        "        \"\"\"\n",
        "        plt.close(\"all\")\n",
        "        sys.stdout = sys.__stdout__\n",
        "\n",
        "\n",
        "finish_time = datetime.now()\n",
        "print((\"\\n\\n\\n\" + \"finish time = {0:02d}:{1:02d}:{2:02.0f}\").format(\n",
        "    finish_time.hour, finish_time.minute, finish_time.second))\n",
        "\n",
        "laps = finish_time - start_time\n",
        "tot_sec = laps.total_seconds()\n",
        "h = int(tot_sec // 3600)\n",
        "m = int((tot_sec % 3600) // 60)\n",
        "s = int(tot_sec - (h * 3600 + m * 60))\n",
        "\n",
        "print(\"total elapsed time = {:02d}:{:2d}:{:2d}\".format(h, m, s))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0texrZhI0uVQ",
        "outputId": "28c76ab5-a505-44e5-ed71-13d4169564f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/637 [00:00<00:39, 15.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Classification Report: tst\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637/637 [00:21<00:00, 29.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "    # Calculate and print the accuracy with desired precision\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "accuracy_str = \"{:.5f}\".format(accuracy)\n",
        "display(\"Accuracy: {}\".format(accuracy_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "czxr4Szx1Bgc",
        "outputId": "8ec49ca0-89f1-4741-8f41-ba450a83729a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Accuracy: 0.97792'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQCNUkPb3n7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now 50% Balaned or Augmneted"
      ],
      "metadata": {
        "id": "OQDmvKf_3iHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into training and testing\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import csv\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from ekg_class import dicts\n",
        "import torch.nn as nn\n",
        "from models_classifier import EcgResNet34\n",
        "from sklearn.metrics import classification_report as report\n",
        "from sklearn.metrics import confusion_matrix as cf_matrix\n",
        "from utils import print_confusion_matrix\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "num2descr, letter2num, letter2descr, num2letter = dicts()\n",
        "start_time = datetime.now()\n",
        "\n",
        "print((\"\\n\" + \"*\" * 61 + \"\\n\\t\\t\\t\\t\\tstart time  {0:02d}:{1:02d}:{2:02.0f}\\n\" + \"*\" * 61).format(\n",
        "    start_time.hour, start_time.minute, start_time.second))\n",
        "\n",
        "drive = \"\"\n",
        "myPath_base = os.path.join(drive, \"\")\n",
        "path_aux = 'paper3_DM\\\\paper3_data'\n",
        "myPath_base = os.path.join(myPath_base, '')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyper-parameters etc.\n",
        "dry_run = False\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_CLASSES = 9\n",
        "tr_ts_ratio = 0.9     # \"training set / test set\" split ratio\n",
        "len_ratio = 0.5\n",
        "\n",
        "if dry_run:\n",
        "    NUM_EPOCHS = 1\n",
        "else:\n",
        "    NUM_EPOCHS = 10\n",
        "#classes2keep = ['/', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "#classes2keep_folder = ['P', 'A', 'L', 'N', 'R', 'f', 'j']\n",
        "\n",
        "classes2keep = ['N','/', 'A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "\n",
        "\n",
        "key_aug = \"aug\"\n",
        "#key_aug = \"notaug\";\n",
        "\n",
        "key_bal = 'balanced'\n",
        "#key_bal = 'imbalanced';\n",
        "\n",
        "# key_case = 'rl'\n",
        "# key_case = \"02\"\n",
        "key_case = \"wgan\"\n",
        "\n",
        "len_ratio = 0.5                  # shorter train sets\n",
        "#len_ratio = 1                  # shorter train sets\n",
        "num_samples = 8000\n",
        "tst_len = 1000\n",
        "print('\\ncase: {}, {}, {}\\n'.format(key_case, key_bal, key_aug))\n",
        "# num_N_samples = int(len_ratio*num_samples)\n",
        "\n",
        "if '.' in str(len_ratio):\n",
        "    len_ratio_str = str(len_ratio).replace('.', '')\n",
        "else:\n",
        "    len_ratio_str = str(len_ratio)\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "path = \"gb_dm_case_{}\".format(key_case)\n",
        "myPath_save = os.path.join(myPath_base, path)\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "brk = 'here'\n",
        "\n",
        "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   begin: From whole database select training and testing set   %%%%%%%%%%%%%%%%%%\n",
        "\n",
        "d_set = \"MIT_BIH/tesing\"\n",
        "myPath_save = os.path.join(myPath_base, \"PycharmProjects\\\\paper2_gen_data\\\\\", d_set,\"50classifierBalanced_wgan_gp_cl_{}_{}\".format(classes2keep_folder,len_ratio))\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "# %%%%%%%%%%%%%%%%       begin MIT-BIH Dataset      %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "with open(os.path.join(myPath_base, \"record_X_y_adapt_win_bef075_aft075_Normalized.json\"), \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stats_all_classes = Counter(np.asarray(data, dtype=object)[:, 2])\n",
        "\n",
        "classes2gb = ['P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "classes2gb_count=['/', 'A', 'L','R', 'f', 'j','F','a']\n",
        "\n",
        "class_counts = {class_label: 0 for class_label in classes2gb_count}\n",
        "\n",
        "# create dictionary of data to be kept\n",
        "vals = []\n",
        "for idx in range(len(classes2keep)):\n",
        "    vals.append([])\n",
        "\n",
        "data2keep_dict = dict(zip(classes2keep, vals))\n",
        "data_train_dict = dict(zip(classes2keep_folder, vals))\n",
        "data_test_dict = dict(zip(classes2keep_folder, vals))\n",
        "count_N = 0\n",
        "for item in data:\n",
        "    if item[2] == 'N'  and count_N < 70000:  # Add condition to limit 'N' data to 10000\n",
        "        data2keep_dict[item[2]].append(item[1])\n",
        "        count_N += 1\n",
        "    elif item[2]!='N' and item[2] in classes2keep:\n",
        "        data2keep_dict[item[2]].append(item[1])\n",
        "        class_counts[item[2]] += 1\n",
        "del data\n",
        "\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "    temp = class_counts['/']\n",
        "    class_counts.pop('/')\n",
        "    class_counts['P'] = temp\n",
        "\n",
        "\n",
        "\n",
        "max_items_per_class= 5000\n",
        "with open(os.path.join(myPath_base, \"gb_together.json\"), \"r\") as f:\n",
        "    data_gb = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for item in data_gb:\n",
        "    if item[1] in classes2gb and class_counts[item[1]] < max_items_per_class:\n",
        "        data2keep_dict[item[1]].append(item[0])\n",
        "        class_counts[item[1]] += 1\n",
        "\n",
        "del data_gb\n",
        "\n",
        "\n",
        "\n",
        "# randomly splitting the dataset into train and test sets\n",
        "for key in data2keep_dict.keys():\n",
        "    val_len = len(data2keep_dict[key])\n",
        "    idx_train = torch.randperm(val_len)[:int(tr_ts_ratio * val_len)]\n",
        "    idx_test = torch.randperm(val_len)[int(tr_ts_ratio * val_len)+1:]\n",
        "    data_train_dict[key] = [data2keep_dict[key][idx] for idx in idx_train]\n",
        "    data_test_dict[key] = [data2keep_dict[key][idx] for idx in idx_test]\n",
        "'''\n",
        "if '/' in classes2keep:\n",
        "    temp = data2keep_dict['/']\n",
        "    data2keep_dict.pop('/')\n",
        "    data2keep_dict['P'] = temp\n",
        "\n",
        "    temp = data_train_dict['/']\n",
        "    data_train_dict.pop('/')\n",
        "    data_train_dict['P'] = temp\n",
        "\n",
        "    temp = data_test_dict['/']\n",
        "    data_test_dict.pop('/')\n",
        "    data_test_dict['P'] = temp\n",
        "\n",
        "'''\n",
        "# create X, y for train and test sets\n",
        "X_train = []\n",
        "y_train = []\n",
        "for key in classes2keep_folder:\n",
        "    temp = data_train_dict[key]\n",
        "    length = int(len_ratio*len(temp))\n",
        "    X_train.extend(temp[:length])\n",
        "    idx = [classes2keep_folder.index(key)] * length\n",
        "    # print('key {}, idx {}'.format(key, idx))\n",
        "    y_train.extend(idx)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for key in classes2keep_folder:\n",
        "    X_test.extend(data_test_dict[key][:])\n",
        "    idx = [classes2keep_folder.index(key)] * len(data_test_dict[key])\n",
        "    y_test.extend(idx)\n",
        "\n",
        "y_test_stat = Counter(y_test)\n",
        "y_train_stat = Counter(y_train)\n",
        "\n",
        "print(\"train: {}\".format(y_train_stat))\n",
        "print(\"test: {}\".format(y_test_stat))\n",
        "\n",
        "a = 0\n",
        "with open(os.path.join(myPath_save, \"X_train.json\"), \"w\") as f:\n",
        "    json.dump(X_train, f)\n",
        "with open(os.path.join(myPath_save, \"y_train.json\"), \"w\") as f:\n",
        "    json.dump(y_train, f)\n",
        "with open(os.path.join(myPath_save, \"X_test.json\"), \"w\") as f:\n",
        "    json.dump(X_test, f)\n",
        "with open(os.path.join(myPath_save, \"y_test.json\"), \"w\") as f:\n",
        "    json.dump(y_test, f)\n",
        "\n",
        "a=0\n",
        "display(\"train: {}\".format(y_train_stat))\n",
        "display(\"test: {}\".format(y_test_stat))\n",
        "trn_set = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "tst_set = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "trn_loader = DataLoader(trn_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "tst_loader = DataLoader(tst_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "brk = 'here'\n",
        "\n",
        "\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%% begin:     Select and Initialize Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "# net = net_cnn(num_classes=NUM_CLASSES).to(device)\n",
        "# net = net_fc(input_size=INPUT_SIZE, num_classes=NUM_CLASSES).to(device)\n",
        "net = EcgResNet34(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "# %%%%%%%%%%%%%%%%%%%% end:     Select and Initialize Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "# %%%%%%%%%%%%%%%%%%%%    begin:  Train Classifier       %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_idx, (inputs, labels) in enumerate(trn_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.reshape(inputs.shape[0], 1, -1)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        scores = net(inputs)\n",
        "        loss = criterion(scores.squeeze(), labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        # running_loss += loss.item()\n",
        "        if batch_idx % 1000 == 0:  # print every 200 mini-batches\n",
        "            now = datetime.now()\n",
        "            display('{:02d}:{:02d}:{:02d}\\t\\tepoch={:4d} / {:4d}\\t\\titer={:5d} / {:5d}\\t\\t\\tloss: {:7.5f}'.\n",
        "                  format(now.hour, now.minute, now.second, epoch, NUM_EPOCHS, batch_idx, len(trn_loader), loss))\n",
        "\n",
        "print('\\n\\tFinished Training\\n')\n",
        "# %%%%%%%%%%%%%%%%%%%%    end  Train Network       %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "\n",
        "display('\\tSaving model ...\\n')\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%    begin save model   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "f_name = \"classifier_{}_{}_{}.pth\".format(key_case, key_bal, key_aug)\n",
        "PATH = os.path.join(myPath_save, f_name)\n",
        "torch.save(net.state_dict(), PATH)\n",
        "# %%%%%%%%%%%%%%%%%%%%    end save model     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "N8QsQNg63pll",
        "outputId": "2792b0e6-b058-487b-ef2a-970afe107d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train: Counter({0: 31500, 3: 3633, 4: 3266, 1: 3162, 2: 2250, 5: 2250, 6: 2250, 7: 2250, 8: 2250})'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test: Counter({0: 6999, 3: 807, 4: 725, 1: 702, 2: 499, 5: 499, 6: 499, 7: 499, 8: 499})'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:15:18\\t\\tepoch=   0 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 2.81353'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:17:22\\t\\tepoch=   0 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.95777'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:19:27\\t\\tepoch=   0 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.17833'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:21:32\\t\\tepoch=   0 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.22351'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:22:10\\t\\tepoch=   1 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.85887'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:24:15\\t\\tepoch=   1 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.75873'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:26:20\\t\\tepoch=   1 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.01522'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:28:25\\t\\tepoch=   1 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.44339'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:29:03\\t\\tepoch=   2 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.73676'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:31:07\\t\\tepoch=   2 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.18937'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:33:12\\t\\tepoch=   2 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.13418'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:35:16\\t\\tepoch=   2 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.03060'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:35:54\\t\\tepoch=   3 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.00632'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:37:58\\t\\tepoch=   3 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.26168'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:40:02\\t\\tepoch=   3 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.15314'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:42:06\\t\\tepoch=   3 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.00314'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:42:43\\t\\tepoch=   4 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.01368'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:44:47\\t\\tepoch=   4 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.01216'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:46:51\\t\\tepoch=   4 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.31878'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:48:55\\t\\tepoch=   4 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.02122'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:49:32\\t\\tepoch=   5 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.01921'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:51:36\\t\\tepoch=   5 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.08811'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:53:40\\t\\tepoch=   5 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 1.29940'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:55:44\\t\\tepoch=   5 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.00823'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:56:21\\t\\tepoch=   6 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.02824'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'06:58:25\\t\\tepoch=   6 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.00100'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:00:29\\t\\tepoch=   6 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.02149'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:02:33\\t\\tepoch=   6 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.02533'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:03:10\\t\\tepoch=   7 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.05974'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:05:14\\t\\tepoch=   7 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.49523'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:07:18\\t\\tepoch=   7 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.00107'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:09:22\\t\\tepoch=   7 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.00616'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'07:09:59\\t\\tepoch=   8 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.00413'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:12:02\\t\\tepoch=   8 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.26005'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:14:05\\t\\tepoch=   8 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.08298'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:16:08\\t\\tepoch=   8 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.16121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:16:45\\t\\tepoch=   9 /   10\\t\\titer=    0 /  3300\\t\\t\\tloss: 0.00575'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:18:48\\t\\tepoch=   9 /   10\\t\\titer= 1000 /  3300\\t\\t\\tloss: 0.29996'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:20:51\\t\\tepoch=   9 /   10\\t\\titer= 2000 /  3300\\t\\t\\tloss: 0.06388'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'07:22:55\\t\\tepoch=   9 /   10\\t\\titer= 3000 /  3300\\t\\t\\tloss: 0.03062'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'\\tSaving model ...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "classes2keep = ['N','/', 'A', 'L', 'R', 'f', 'j','F','a']\n",
        "classes2keep_folder = ['N','P', 'A', 'L','R', 'f', 'j','F','a']\n",
        "len_ratio=0.5\n",
        "# Specify the path where the JSON files are saved\n",
        "d_set = \"MIT_BIH/tesing\"\n",
        "myPath_save = os.path.join( \"PycharmProjects\\\\paper2_gen_data\\\\\", d_set,\"50classifierBalanced_wgan_gp_cl_{}_{}\".format(classes2keep_folder,len_ratio))\n",
        "os.makedirs(myPath_save, exist_ok=True)\n",
        "\n",
        "# Load data into X_train\n",
        "with open(os.path.join(myPath_save, \"X_train.json\"), \"r\") as f:\n",
        "    X_train = json.load(f)\n",
        "\n",
        "# Load data into X_test\n",
        "with open(os.path.join(myPath_save, \"X_test.json\"), \"r\") as f:\n",
        "    X_test = json.load(f)\n",
        "\n",
        "with open(os.path.join(myPath_save, \"y_train.json\"), \"r\") as f:\n",
        "    y_train = json.load(f)\n",
        "\n",
        "with open(os.path.join(myPath_save, \"y_test.json\"), \"r\") as f:\n",
        "    y_test = json.load(f)\n"
      ],
      "metadata": {
        "id": "-IpajRoEbqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%%%%%%%%%%%%%%%%%%%    load trained classifier     %%%%%%%%%%%%%%%%%%%%%\n",
        "path = \"gb_dm_case_{}\".format(key_case)\n",
        "myPath = os.path.join(myPath_base, path)\n",
        "classifier_name = \"classifier_{}_{}_{}.pth\".format(key_case, key_bal, key_aug)\n",
        "net.load_state_dict(torch.load(os.path.join(myPath_save, classifier_name)))\n",
        "\n",
        "brk = 'here'\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%    print Classification Report to file      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "trntst = ['tst']\n",
        "for key_trntst in trntst:\n",
        "    print('Preparing Classification Report: {}'.format(key_trntst))\n",
        "    path = os.path.join(myPath_save, 'classification_report_{}_{}_{}_{}.txt'.\n",
        "                        format(key_trntst, key_case, key_bal, key_aug))\n",
        "    with open(path, 'w') as sys.stdout:\n",
        "        y_scores = []\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        exec('loader = {}_loader'.format(key_trntst))\n",
        "        for (inputs, labels) in tqdm(loader):\n",
        "            inputs = inputs.reshape(inputs.shape[0], 1, -1).to(device)\n",
        "            labels = labels.to(device)\n",
        "            temp = net(inputs)\n",
        "            output = temp.max(dim=1)\n",
        "\n",
        "            y_true.extend(labels.data.tolist())\n",
        "            y_pred.extend(output.indices.tolist())\n",
        "            y_scores.extend((F.softmax(temp, dim=1).tolist()))\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"%\" * 20 + \"\\tClassification Report ({} Set, {}, {}, {})\\t\".\n",
        "              format(key_trntst, key_case, key_bal, key_aug) + \"%\" * 20)\n",
        "        print(\"Classes:                             {}\".format(classes2keep_folder))\n",
        "        print(\"Classifier Model:                    {}\".format(net._get_name()))\n",
        "        print(f\"number of epochs:                    {NUM_EPOCHS}\\n\")\n",
        "        print(\"Train set length:                    {}\".format(len(trn_set)))\n",
        "        print(\"Test set length:                     {}\".format(len(tst_set)))\n",
        "        print(\"Train set reduction length ratio:    {}\\t\".format(len_ratio_str))\n",
        "        print('\\nClassification Report:')\n",
        "        print(report(y_true, y_pred, target_names=classes2keep_folder))\n",
        "        print(\"\\nConfusion Matrix:\\n {}\".format(cf_matrix(y_true, y_pred)))\n",
        "        \"\"\"\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true, np.asarray(y_scores)[:, 1])\n",
        "        #here auc function won't work as it works for binary classifier\n",
        "        pr_recall_auc_score = auc(recall, precision)\n",
        "        print('\\nPrecision-Recall AUC Score:        {:6.4f}'.format(pr_recall_auc_score))\n",
        "        rocauc_score = roc_auc_score(y_true, np.asarray(y_scores)[:, 1])\n",
        "        print('\\nROC AUC Score:                     {:6.4f}'.format(rocauc_score))\n",
        "\n",
        "        print_confusion_matrix(cf_matrix(y_true, y_pred), class_names=classes2keep_folder,\n",
        "                               fig_name=\"Conf. Matrix_{}_{}_{}_{}\".format(key_trntst, key_case, key_bal, key_aug))\n",
        "        plt.savefig(os.path.join(myPath_save, \"cfmx_{}_{}_{}_{}.png\".format(key_trntst, key_case, key_bal, key_aug)))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "\n",
        "        plt.plot(precision, recall)\n",
        "        plt.title(\"Precision - Recall Curve ({}, {}, {}, {})\".format(key_trntst, key_case, key_bal, key_aug))\n",
        "        plt.grid()\n",
        "        plt.savefig(os.path.join(myPath_save, \"prec_recall_curve_{}_{}_{}_{}.png\".\n",
        "                                 format(key_trntst, key_case, key_bal, key_aug)))\n",
        "        \"\"\"\n",
        "        plt.close(\"all\")\n",
        "        sys.stdout = sys.__stdout__\n",
        "\n",
        "\n",
        "finish_time = datetime.now()\n",
        "print((\"\\n\\n\\n\" + \"finish time = {0:02d}:{1:02d}:{2:02.0f}\").format(\n",
        "    finish_time.hour, finish_time.minute, finish_time.second))\n",
        "\n",
        "laps = finish_time - start_time\n",
        "tot_sec = laps.total_seconds()\n",
        "h = int(tot_sec // 3600)\n",
        "m = int((tot_sec % 3600) // 60)\n",
        "s = int(tot_sec - (h * 3600 + m * 60))\n",
        "\n",
        "print(\"total elapsed time = {:02d}:{:2d}:{:2d}\".format(h, m, s))\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "    # Calculate and print the accuracy with desired precision\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "accuracy_str = \"{:.5f}\".format(accuracy)\n",
        "display(\"Accuracy: {}\".format(accuracy_str))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fZIyTbIbveqf",
        "outputId": "3f7123b9-bea2-472f-e81d-2812b467725b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 733/733 [00:24<00:00, 29.36it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Accuracy: 0.97655'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_true , y_pred)\n",
        "\n",
        "classes = ['N', 'P', 'A', 'L', 'R', 'f', 'j', 'F', 'a']\n",
        "\n",
        "\n",
        "# Calculate percentages\n",
        "cm_percentages = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Create a pandas DataFrame for the confusion matrix\n",
        "cm_df = pd.DataFrame(cm_percentages, index=classes, columns=classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Percentages)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YQoeMxfA0WUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"new\")"
      ],
      "metadata": {
        "id": "ac-lWC5G4ylS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}